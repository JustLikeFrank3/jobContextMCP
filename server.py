#!/usr/bin/env python3
"""
Job Search MCP Server for Frank MacBride
-----------------------------------------
Provides tools for:
  - Job hunt status tracking
  - Resume / cover letter context generation
  - Job fitment assessment
  - Interview & LeetCode prep
  - sPiCam project skill scanning
  - Mental health check-in logging
  - Personal story / context library (v3)
  - Tone ingestion + voice profile (v3)
"""

import json
import os
import datetime
from pathlib import Path

from mcp.server.fastmcp import FastMCP

# ‚îÄ‚îÄ‚îÄ CONFIGURATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Paths are loaded from config.json (gitignored) sitting next to this file.
# Copy config.example.json ‚Üí config.json and fill in your own paths.

_HERE = Path(__file__).parent


def _load_config() -> dict:
    config_path = _HERE / "config.json"
    if not config_path.exists():
        raise FileNotFoundError(
            f"config.json not found at {config_path}\n"
            "Copy config.example.json ‚Üí config.json and fill in your paths."
        )
    return json.loads(config_path.read_text(encoding="utf-8"))


_cfg = _load_config()

RESUME_FOLDER   = Path(_cfg["resume_folder"])
LEETCODE_FOLDER = Path(_cfg["leetcode_folder"])
SPICAM_FOLDER   = Path(_cfg["spicam_folder"])
DATA_FOLDER     = Path(_cfg["data_folder"])

STATUS_FILE           = DATA_FOLDER / "status.json"
HEALTH_LOG_FILE       = DATA_FOLDER / "mental_health_log.json"
PERSONAL_CONTEXT_FILE = DATA_FOLDER / "personal_context.json"
TONE_FILE             = DATA_FOLDER / "tone_samples.json"

MASTER_RESUME       = RESUME_FOLDER / _cfg["master_resume_path"]
LEETCODE_CHEATSHEET = LEETCODE_FOLDER / _cfg["leetcode_cheatsheet_path"]
QUICK_REFERENCE     = LEETCODE_FOLDER / _cfg["quick_reference_path"]

# ‚îÄ‚îÄ‚îÄ SERVER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

mcp = FastMCP(
    "job-search-as",
    instructions=(
        "You are Frank MacBride's personal job search assistant. "
        "You have direct filesystem access to his resume materials, job hunt status, "
        "and interview prep files. Use the available tools to retrieve context before "
        "generating resumes, cover letters, prep docs, or assessments. "
        "Always read the master resume before generating any application material."
    ),
)

# ‚îÄ‚îÄ‚îÄ HELPERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _read(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"[Error reading {path.name}: {e}]"


def _load_json(path: Path, default):
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return default


def _save_json(path: Path, data) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


def _now() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M")


# ‚îÄ‚îÄ‚îÄ TOOLS: JOB HUNT STATUS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def get_job_hunt_status() -> str:
    """
    Returns the current status of all tracked job applications with
    next steps, contacts, and notes.
    """
    data = _load_json(STATUS_FILE, {"applications": []})
    apps = data.get("applications", [])
    if not apps:
        return "No applications tracked yet. Use update_application() to add one."

    lines = [
        "‚ïê‚ïê‚ïê JOB HUNT STATUS ‚ïê‚ïê‚ïê",
        f"Last updated: {data.get('last_updated', 'unknown')}",
        "",
    ]
    for app in apps:
        lines.append(f"‚ñ† {app['company']} ‚Äî {app['role']}")
        lines.append(f"  Status:       {app['status']}")
        lines.append(f"  Last update:  {app.get('last_updated', '‚Äî')}")
        if app.get("next_steps"):
            lines.append(f"  Next steps:   {app['next_steps']}")
        if app.get("contact"):
            lines.append(f"  Contact:      {app['contact']}")
        if app.get("notes"):
            lines.append(f"  Notes:        {app['notes']}")
        lines.append("")
    return "\n".join(lines)


@mcp.tool()
def update_application(
    company: str,
    role: str,
    status: str,
    next_steps: str = "",
    contact: str = "",
    notes: str = "",
) -> str:
    """
    Add or update a job application in the tracker.

    status values: applied | phone_screen | technical_screen | coding_assessment
                   system_design | onsite | offer | rejected | withdrawn | waiting
    """
    data = _load_json(STATUS_FILE, {"applications": []})
    apps: list = data.setdefault("applications", [])

    existing = next(
        (a for a in apps
         if a["company"].lower() == company.lower()
         and a["role"].lower() == role.lower()),
        None,
    )
    # Fallback: match on company only if exact role not found
    if existing is None:
        existing = next(
            (a for a in apps if a["company"].lower() == company.lower()), None
        )
    if existing:
        existing.update(
            role=role,
            status=status,
            next_steps=next_steps,
            contact=contact,
            notes=notes,
            last_updated=_now(),
        )
        action = "Updated"
    else:
        apps.append(
            dict(
                company=company,
                role=role,
                status=status,
                next_steps=next_steps,
                contact=contact,
                notes=notes,
                applied_date=_now(),
                last_updated=_now(),
            )
        )
        action = "Added"

    data["last_updated"] = _now()
    _save_json(STATUS_FILE, data)
    return f"‚úì {action}: {company} ‚Äî {role} ({status})"


# ‚îÄ‚îÄ‚îÄ TOOLS: RESUME CONTENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def read_master_resume() -> str:
    """
    Returns the full MASTER SOURCE resume with all metrics, achievements, and
    personal context notes. Always call this before generating any resume or
    cover letter.
    """
    return _read(MASTER_RESUME)


@mcp.tool()
def list_existing_materials(company: str = "") -> str:
    """
    Lists resumes and cover letters already generated in this workspace.
    Optionally filter by company name.
    """
    optimized_dir    = RESUME_FOLDER / _cfg["optimized_resumes_dir"]
    cover_letter_dir = RESUME_FOLDER / _cfg["cover_letters_dir"]

    def _list_dir(d: Path, label: str) -> list[str]:
        if not d.exists():
            return [f"  (folder not found: {d.name})"]
        files = sorted(
            f.name for f in d.iterdir()
            if f.suffix in (".txt", ".md") and "MASTER" not in f.name
            and (not company or company.lower() in f.name.lower())
        )
        out = [f"\n‚ïê‚ïê {label} ({len(files)}) ‚ïê‚ïê"]
        out += [f"  {f}" for f in files] or ["  (none found)"]
        return out

    lines = []
    lines += _list_dir(optimized_dir,    "RESUMES")
    lines += _list_dir(cover_letter_dir, "COVER LETTERS")
    return "\n".join(lines)


@mcp.tool()
def read_existing_resume(filename: str) -> str:
    """
    Read the content of a specific resume file.
    filename should be just the base filename (no path) from 01-Current-Optimized/.
    Use list_existing_materials() to see available files.
    """
    path = RESUME_FOLDER / _cfg["optimized_resumes_dir"] / filename
    if not path.exists():
        return f"Not found: {filename}\nUse list_existing_materials() to see available resumes."
    return _read(path)


@mcp.tool()
def read_reference_file(filename: str) -> str:
    """
    Read a file from the 06-Reference-Materials/ folder.
    Useful files: 'Frank MacBride Resume - Template Format.txt',
                  'Skills - 10% Shorter.txt',
                  'GM Recognition Awards, Feedback_Received.txt'
    """
    ref_dir = RESUME_FOLDER / _cfg["reference_materials_dir"]
    path = ref_dir / filename
    if not path.exists():
        available = sorted(
            f.name for f in ref_dir.iterdir()
        ) if ref_dir.exists() else []
        return f"Not found: {filename}\nAvailable: {available}"
    return _read(path)


# ‚îÄ‚îÄ‚îÄ TOOLS: FITMENT & STRATEGY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def assess_job_fitment(company: str, role: str, job_description: str) -> str:
    """
    Provides everything needed to assess how well Frank's background maps to a
    job description. Returns the master resume + JD packaged for analysis.

    After calling this tool, produce:
      1. Overall fit score (0‚Äì100%)
      2. Key strengths matching requirements
      3. Gaps / risks
      4. Suggested resume emphasis (which bullets to lead with)
      5. Top 3‚Äì5 STAR stories from Frank's background most relevant here
      6. Questions to probe / clarify before applying
    """
    master = _read(MASTER_RESUME)
    return (
        f"‚ïê‚ïê‚ïê FITMENT ASSESSMENT ‚ïê‚ïê‚ïê\n"
        f"Company: {company}\n"
        f"Role:    {role}\n\n"
        f"‚îÄ‚îÄ‚îÄ‚îÄ JOB DESCRIPTION ‚îÄ‚îÄ‚îÄ‚îÄ\n{job_description}\n\n"
        f"‚îÄ‚îÄ‚îÄ‚îÄ FRANK'S MASTER RESUME ‚îÄ‚îÄ‚îÄ‚îÄ\n{master}"
    )


@mcp.tool()
def get_customization_strategy(role_type: str) -> str:
    """
    Returns the recommended resume emphasis strategy for a given role type.

    role_type options:
      testing | cloud | data_engineering | backend | fullstack | ai_innovation | iot
    """
    strategies = {
        "testing": (
            "Lead with JUnit/Mockito/Selenium expertise, 80%+ coverage metrics, TDD practices. "
            "Feature the 'prevented production defects' story. "
            "Highlight Karma/Jest for frontend testing."
        ),
        "cloud": (
            "Lead with Azure Container Apps, Terraform IaC, zero-downtime PCF‚ÜíOCF‚ÜíAzure migration. "
            "Emphasize containerization, CI/CD pipelines, and infrastructure-as-code."
        ),
        "data_engineering": (
            "Lead with IBM DataStage, PySpark ETL pipelines, Oracle‚ÜíPostgreSQL migration. "
            "Emphasize data modeling, multi-source integration, and warehouse work."
        ),
        "backend": (
            "Lead with microservices architecture, event-driven pub/sub, Spring Boot, "
            "98% SLA compliance on production forecasting app. "
            "Emphasize distributed systems debugging, on-call rotation, observability."
        ),
        "fullstack": (
            "Lead with Java/Spring Boot + Angular/TypeScript full ownership. "
            "Emphasize API design, end-to-end modernization, cross-functional product work."
        ),
        "ai_innovation": (
            "Lead with GitHub Copilot champion story: 35% org adoption, 3.5x target. "
            "Emphasize technical evangelism, AI tooling adoption, and measurable team impact."
        ),
        "iot": (
            "Lead with IoT Engineering degree, RetrosPiCam project (FastAPI + Raspberry Pi + servo HAT), "
            "hardware/software integration, and Azure edge/cloud connectivity. "
            "Tie in LiveVox latency work (2.8ms web, 12.7ms iOS) as embedded-adjacent."
        ),
    }
    result = strategies.get(role_type.lower())
    if result:
        return f"Strategy for '{role_type}':\n\n{result}"
    return (
        f"Unknown role type: '{role_type}'\n"
        f"Available options: {', '.join(strategies)}"
    )


# ‚îÄ‚îÄ‚îÄ TOOLS: INTERVIEW & LEETCODE PREP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def get_interview_quick_reference() -> str:
    """
    Returns Frank's interview day quick reference: STAR stories, system design
    framework, behavioral talking points, questions to ask. Best used before
    any behavioral or system design interview.
    """
    return _read(QUICK_REFERENCE)


@mcp.tool()
def get_leetcode_cheatsheet(section: str = "") -> str:
    """
    Returns the full LeetCode / algorithm cheatsheet, or a specific section.

    section options (leave empty for full cheatsheet):
      arrays | linkedlist | trees | graphs | stacks | dp | system_design | strings
    """
    content = _read(LEETCODE_CHEATSHEET)
    if not section:
        return content

    # Try to isolate the requested section by header matching
    lines   = content.split("\n")
    result  = []
    inside  = False
    target  = section.lower()

    for line in lines:
        stripped = line.lstrip("#").strip().lower()
        is_header = line.startswith("#")

        if is_header and target in stripped:
            inside = True
        elif is_header and inside and target not in stripped:
            # Hit a same-or-higher-level header ‚Äî stop
            if line.startswith("# ") or (line.startswith("## ") and not target in stripped):
                break

        if inside:
            result.append(line)

    if result:
        return "\n".join(result)
    return f"Section '{section}' not found. Returning full cheatsheet.\n\n{content}"


@mcp.tool()
def generate_interview_prep_context(
    company: str,
    role: str,
    stage: str = "phone_screen",
    job_description: str = "",
) -> str:
    """
    Packages Frank's master resume + quick reference for interview prep.
    Returns structured context for generating a company-specific prep document.

    stage options: phone_screen | technical_screen | coding_assessment
                   system_design | onsite | behavioral
    """
    master    = _read(MASTER_RESUME)
    quick_ref = _read(QUICK_REFERENCE)

    desc_block = f"\n‚îÄ‚îÄ‚îÄ‚îÄ JOB DESCRIPTION ‚îÄ‚îÄ‚îÄ‚îÄ\n{job_description}" if job_description else ""

    return (
        f"‚ïê‚ïê‚ïê INTERVIEW PREP CONTEXT ‚ïê‚ïê‚ïê\n"
        f"Company: {company}\n"
        f"Role:    {role}\n"
        f"Stage:   {stage}\n"
        f"{desc_block}\n\n"
        f"‚îÄ‚îÄ‚îÄ‚îÄ FRANK'S MASTER RESUME ‚îÄ‚îÄ‚îÄ‚îÄ\n{master}\n\n"
        f"‚îÄ‚îÄ‚îÄ‚îÄ QUICK REFERENCE / STAR STORIES ‚îÄ‚îÄ‚îÄ‚îÄ\n{quick_ref}\n\n"
        f"Use the above to produce:\n"
        f"  1. Top 5 things Frank must communicate for THIS role/stage\n"
        f"  2. Anticipated questions + suggested STAR responses\n"
        f"  3. Technical topics to review (if applicable)\n"
        f"  4. Smart questions for Frank to ask the interviewer\n"
        f"  5. Any gaps to proactively address\n"
        f"  6. Confidence anchors ‚Äî Frank's strongest achievements relevant here\n"
    )


@mcp.tool()
def get_existing_prep_file(company: str) -> str:
    """
    Looks for an existing interview prep file for the given company.
    Searches the Resume 2025 folder recursively for matching .txt and .md files.
    """
    matches = sorted(
        f for f in RESUME_FOLDER.rglob("*")
        if f.suffix in (".txt", ".md")
        and company.lower() in f.name.lower()
        and any(kw in f.name.lower() for kw in ("prep", "interview", "call", "assessment"))
    )
    if not matches:
        return f"No existing prep files found for '{company}'."

    lines = [f"Found {len(matches)} prep file(s) for '{company}':\n"]
    for m in matches:
        lines.append(f"‚îÄ‚îÄ‚îÄ‚îÄ {m.name} ‚îÄ‚îÄ‚îÄ‚îÄ")
        lines.append(_read(m))
        lines.append("")
    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ TOOLS: RETROSPICAM SKILL SCANNER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def scan_spicam_for_skills() -> str:
    """
    Scans the RetrosPiCam project codebase and returns:
      - Technologies / patterns detected
      - Suggested resume bullets
      - New skills not yet in the master resume
    """
    if not SPICAM_FOLDER.exists():
        return f"RetrosPiCam folder not found at: {SPICAM_FOLDER}"

    tech_found: set[str] = set()
    file_inventory: list[str] = []

    SKIP_DIRS = {".git", "__pycache__", "node_modules", "venv", ".venv", "env", ".expo", "build", "dist"}

    for root, dirs, files in os.walk(SPICAM_FOLDER):
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.startswith(".")]

        for fname in files:
            fpath = Path(root) / fname
            rel   = str(fpath.relative_to(SPICAM_FOLDER))
            ext   = fpath.suffix.lower()
            file_inventory.append(rel)

            try:
                text = fpath.read_text(encoding="utf-8", errors="ignore").lower()
            except Exception:
                continue

            # ‚îÄ‚îÄ Python ‚îÄ‚îÄ
            if ext == ".py":
                tech_found.add("Python")
                if "fastapi"          in text: tech_found.add("FastAPI")
                if "pydantic"         in text: tech_found.add("Pydantic")
                if "azure"            in text: tech_found.add("Azure Blob Storage")
                if "async def"        in text: tech_found.add("Python async/await")
                if "websocket"        in text: tech_found.add("WebSockets")
                if "pytest"           in text: tech_found.add("pytest")
                if "systemd"          in text: tech_found.add("systemd / Linux services")
                if "gpio"             in text or "rpi" in text: tech_found.add("Raspberry Pi GPIO")
                if "servo"            in text or "adafruit" in text: tech_found.add("Servo / Adafruit HAT")
                if "range"            in text and "http" in text: tech_found.add("HTTP Range requests")
                if "jwt"              in text or "bearer" in text: tech_found.add("JWT authentication")
                if "docker"           in text: tech_found.add("Docker")
                if "retention"        in text: tech_found.add("Automated retention policies")

            # ‚îÄ‚îÄ TypeScript / React Native ‚îÄ‚îÄ
            elif ext in (".ts", ".tsx", ".js", ".jsx"):
                tech_found.add("TypeScript/JavaScript")
                if "react-native"     in text or "react native" in text: tech_found.add("React Native")
                if "expo"             in text: tech_found.add("Expo")
                if "testflight"       in text: tech_found.add("iOS TestFlight deployment")

            # ‚îÄ‚îÄ Swift ‚îÄ‚îÄ
            elif ext == ".swift":
                tech_found.add("Swift / iOS")

            # ‚îÄ‚îÄ Infrastructure ‚îÄ‚îÄ
            elif fname == "Dockerfile":
                tech_found.add("Docker")
            elif fname in ("docker-compose.yml", "docker-compose.yaml"):
                tech_found.add("Docker Compose")
            elif ext in (".tf", ".tfvars"):
                tech_found.add("Terraform IaC")

    already_on_resume = {
        "Python", "FastAPI", "Azure Blob Storage", "React Native", "Pydantic",
        "iOS TestFlight deployment", "HTTP Range requests",
    }
    new_skills = sorted(tech_found - already_on_resume)

    lines = [
        "‚ïê‚ïê‚ïê RETROSPICAM SKILL SCAN ‚ïê‚ïê‚ïê",
        f"Files scanned: {len(file_inventory)}",
        "",
        "‚îÄ‚îÄ All Technologies Detected ‚îÄ‚îÄ",
    ]
    for t in sorted(tech_found):
        marker = "  ‚úì" if t in already_on_resume else "  ‚òÖ NEW"
        lines.append(f"{marker}  {t}")

    lines += [
        "",
        "‚îÄ‚îÄ New Skills Not Yet on Master Resume ‚îÄ‚îÄ",
    ]
    lines += [f"  ‚Ä¢ {s}" for s in new_skills] or ["  (none ‚Äî master resume is up to date)"]

    lines += [
        "",
        "‚îÄ‚îÄ Suggested Resume Bullets ‚îÄ‚îÄ",
        "  ‚Ä¢ Built production IoT camera system integrating Raspberry Pi hardware, "
        "    servo HAT, Python/FastAPI backend, and React Native mobile app",
    ]
    if "Pydantic"              in tech_found: lines.append("  ‚Ä¢ Designed type-safe API layer with FastAPI + Pydantic models")
    if "Python async/await"    in tech_found: lines.append("  ‚Ä¢ Implemented async Python services for concurrent hardware + network I/O")
    if "Azure Blob Storage"    in tech_found: lines.append("  ‚Ä¢ Integrated Azure Blob Storage with automated 7-day retention management")
    if "HTTP Range requests"   in tech_found: lines.append("  ‚Ä¢ Enabled on-demand video streaming via HTTP Range request support")
    if "systemd / Linux services" in tech_found: lines.append("  ‚Ä¢ Configured systemd service for reliable auto-start on embedded Linux")
    if "WebSockets"            in tech_found: lines.append("  ‚Ä¢ Delivered real-time camera stream via WebSocket connections")
    if "JWT authentication"    in tech_found: lines.append("  ‚Ä¢ Secured API endpoints with JWT bearer-token authentication")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ TOOLS: MENTAL HEALTH CHECK-INS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def log_mental_health_checkin(
    mood: str,
    energy: int,
    notes: str = "",
    productive: bool = False,
) -> str:
    """
    Log a mental health check-in entry.

    mood:      e.g. 'good' | 'anxious' | 'depressed' | 'hyperfocus' | 'stable' | 'low' | 'motivated'
    energy:    1‚Äì10 scale (1 = can barely get out of bed, 10 = hyperfocus mode)
    notes:     optional free-text context
    productive: did you get meaningful work done today?
    """
    data = _load_json(HEALTH_LOG_FILE, {"entries": []})
    entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "date":      datetime.date.today().isoformat(),
        "mood":      mood,
        "energy":    max(1, min(10, int(energy))),
        "productive": bool(productive),
        "notes":     notes,
    }
    data.setdefault("entries", []).append(entry)
    _save_json(HEALTH_LOG_FILE, data)

    energy_int = entry["energy"]
    if energy_int <= 3 or mood in ("depressed", "low"):
        guidance = (
            "Low energy logged. Small wins count ‚Äî "
            "even one LeetCode problem or one email sent is real progress. "
            "You're still moving, even on hard days."
        )
    elif mood == "hyperfocus" or energy_int >= 8:
        guidance = (
            "High energy logged. Good time for deep work. "
            "Just remember to eat, hydrate, and step away before burnout hits."
        )
    else:
        guidance = "Logged. You're doing the work, even when it's hard."

    return f"Check-in saved ({entry['date']}, energy {energy_int}/10, mood: {mood}).\n{guidance}"


@mcp.tool()
def get_mental_health_log(days: int = 14) -> str:
    """
    Returns mental health log entries from the last N days (default: 14).
    Includes average energy trend and any notable patterns.
    """
    data    = _load_json(HEALTH_LOG_FILE, {"entries": []})
    entries = data.get("entries", [])

    cutoff  = (datetime.date.today() - datetime.timedelta(days=days)).isoformat()
    recent  = [e for e in entries if e.get("date", "") >= cutoff]

    if not recent:
        return f"No check-ins logged in the past {days} days."

    lines = [f"‚ïê‚ïê‚ïê MENTAL HEALTH LOG (last {days} days) ‚ïê‚ïê‚ïê", ""]
    for e in reversed(recent):
        eng = e.get("energy", 0)
        bar = "üü•" if eng <= 3 else "üü®" if eng <= 6 else "üü©"
        prod = "‚úì" if e.get("productive") else "‚Äì"
        lines.append(
            f"{e['date']}  {bar}  mood: {e['mood']:<12}  energy: {eng}/10  productive: {prod}"
        )
        if e.get("notes"):
            lines.append(f"          ‚Ü≥ {e['notes']}")

    avg = sum(e.get("energy", 0) for e in recent) / len(recent)
    lines += ["", f"Average energy over {days} days: {avg:.1f}/10"]

    # Trend note
    if avg <= 4:
        lines.append("‚ö†  Trend: extended low-energy period. Consider reaching out for support.")
    elif avg >= 7:
        lines.append("‚úì  Trend: strong energy. Keep the momentum going.")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ TOOLS: PERSONAL CONTEXT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def log_personal_story(
    story: str,
    tags: list[str],
    people: list[str] = [],
    title: str = "",
) -> str:
    """
    Log a personal story, memory, or context detail about Frank.
    These accumulate over time and are retrieved to enrich cover letters,
    dedications, behavioral answers, and any writing that should sound like Frank.

    tags examples: family | motivation | music | career | friendship | identity | humor
    people: names of people relevant to the story (e.g. ["Sean Evans", "Grandpa Frank"])
    title: optional short label for the story (auto-generated from first 60 chars if omitted)
    """
    data = _load_json(PERSONAL_CONTEXT_FILE, {"stories": []})
    entry = {
        "id": len(data["stories"]) + 1,
        "timestamp": datetime.datetime.now().isoformat(),
        "title": title or (story[:60] + ("..." if len(story) > 60 else "")),
        "story": story,
        "tags": [t.lower().strip() for t in tags],
        "people": people,
    }
    data["stories"].append(entry)
    _save_json(PERSONAL_CONTEXT_FILE, data)
    return f"\u2713 Story logged (#{entry['id']}): {entry['title']}"


@mcp.tool()
def get_personal_context(tag: str = "", person: str = "") -> str:
    """
    Retrieve Frank's logged personal stories and context.
    Filter by tag or person name, or leave both empty to get all stories.

    Use this before generating cover letters, dedications, or any writing
    that should feel personal rather than generic.
    """
    data = _load_json(PERSONAL_CONTEXT_FILE, {"stories": []})
    stories = data.get("stories", [])

    if tag:
        stories = [s for s in stories if tag.lower() in s.get("tags", [])]
    if person:
        stories = [s for s in stories if any(person.lower() in p.lower() for p in s.get("people", []))]

    if not stories:
        qualifier = f" for tag '{tag}'" if tag else ""
        qualifier += f" for person '{person}'" if person else ""
        return f"No personal stories found{qualifier}."

    lines = [f"\u2550\u2550\u2550 PERSONAL CONTEXT ({len(stories)} stories) \u2550\u2550\u2550", ""]
    for s in stories:
        lines.append(f"\u25aa #{s['id']} \u2014 {s['title']}")
        lines.append(f"  Tags:   {', '.join(s.get('tags', []))}")
        if s.get("people"):
            lines.append(f"  People: {', '.join(s['people'])}")
        lines.append(f"  {s['story']}")
        lines.append("")
    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ TOOLS: TONE INGESTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def log_tone_sample(
    text: str,
    source: str,
    context: str = "",
) -> str:
    """
    Ingest a writing sample to capture Frank's tone and voice.
    Over time, the AI uses these to write in Frank's style rather than generic AI prose.

    source: where the sample came from
            e.g. "cover letter Ford", "text to Jessica", "LinkedIn message to Cheyenne"
    context: optional note about the situation or mood when written
    """
    data = _load_json(TONE_FILE, {"samples": []})
    entry = {
        "id": len(data["samples"]) + 1,
        "timestamp": datetime.datetime.now().isoformat(),
        "source": source,
        "context": context,
        "text": text,
        "word_count": len(text.split()),
    }
    data["samples"].append(entry)
    _save_json(TONE_FILE, data)
    return f"\u2713 Tone sample logged (#{entry['id']}, {entry['word_count']} words from '{source}')"


@mcp.tool()
def get_tone_profile() -> str:
    """
    Returns all ingested tone samples so the AI can write in Frank's voice.
    Call this before drafting any email, cover letter, LinkedIn message,
    or other communication that needs to sound like Frank ‚Äî not like an AI.
    """
    data = _load_json(TONE_FILE, {"samples": []})
    samples = data.get("samples", [])

    if not samples:
        return (
            "No tone samples logged yet.\n"
            "Use log_tone_sample() to ingest writing samples ‚Äî cover letters, "
            "messages, anything Frank actually wrote."
        )

    total_words = sum(s.get("word_count", 0) for s in samples)
    lines = [
        f"\u2550\u2550\u2550 TONE PROFILE ({len(samples)} samples, {total_words} total words) \u2550\u2550\u2550",
        "Use these samples to calibrate Frank's voice before writing anything.",
        "",
    ]
    for s in samples:
        lines.append(f"\u2500\u2500 Sample #{s['id']} | {s['source']} \u2500\u2500")
        if s.get("context"):
            lines.append(f"Context: {s['context']}")
        lines.append(s["text"])
        lines.append("")
    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ TOOLS: RAG SEARCH ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@mcp.tool()
def search_materials(query: str, category: str = "") -> str:
    """
    Semantic search across all indexed job search materials.
    Much smarter than reading whole files ‚Äî returns only the most relevant chunks.

    category options (leave empty to search all):
      resume | cover_letters | leetcode | interview_prep | reference

    Examples:
      search_materials("PostgreSQL migration zero downtime")
      search_materials("sliding window pattern", category="leetcode")
      search_materials("behavioral leadership story", category="interview_prep")
    """
    try:
        from rag import search, format_results
        hits = search(query, category=category or None, n_results=6)
        return format_results(hits, f'Results for: "{query}"')
    except Exception as e:
        if "openai_api_key" in str(e).lower() or "not set" in str(e).lower():
            return (
                "RAG search requires an OpenAI API key.\n"
                "Add 'openai_api_key' to config.json, then run reindex_materials()."
            )
        return f"Search error: {e}\nTry running reindex_materials() first."


@mcp.tool()
def reindex_materials() -> str:
    """
    (Re)build the RAG index from all job search materials.
    Run this once after setup, and again whenever you add new resumes,
    cover letters, or prep files. Takes ~30-60 seconds.
    """
    try:
        from rag import build_index
        counts = build_index(verbose=False)
        total  = sum(counts.values())
        lines  = [f"‚úì Index built. {total} total chunks indexed:", ""]
        for cat, count in counts.items():
            lines.append(f"  {cat:<16} {count} chunks")
        lines += ["", "You can now use search_materials() for semantic search."]
        return "\n".join(lines)
    except Exception as e:
        if "openai_api_key" in str(e).lower() or "not set" in str(e).lower():
            return (
                "Indexing requires an OpenAI API key.\n"
                "Add 'openai_api_key' to config.json and try again."
            )
        return f"Indexing error: {e}"


# ‚îÄ‚îÄ‚îÄ ENTRY POINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    mcp.run()
